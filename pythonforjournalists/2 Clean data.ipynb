{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Clean-data-with-Python-Pandas\" data-toc-modified-id=\"Clean-data-with-Python-Pandas-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Clean data with Python Pandas</a></span><ul class=\"toc-item\"><li><span><a href=\"#About-Jupyter-Notebooks-and-Pandas\" data-toc-modified-id=\"About-Jupyter-Notebooks-and-Pandas-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>About Jupyter Notebooks and Pandas</a></span></li></ul></li><li><span><a href=\"#Getting-started\" data-toc-modified-id=\"Getting-started-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Getting started</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Import-data\" data-toc-modified-id=\"Import-data-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Import data</a></span></li></ul></li><li><span><a href=\"#Explore-data\" data-toc-modified-id=\"Explore-data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Explore data</a></span></li></ul></li><li><span><a href=\"#Clean-data\" data-toc-modified-id=\"Clean-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Clean data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cleaning-strings\" data-toc-modified-id=\"Cleaning-strings-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Cleaning strings</a></span></li><li><span><a href=\"#Delete-columns\" data-toc-modified-id=\"Delete-columns-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Delete columns</a></span></li><li><span><a href=\"#Renaming-columns\" data-toc-modified-id=\"Renaming-columns-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Renaming columns</a></span></li><li><span><a href=\"#Clean-up-column-names\" data-toc-modified-id=\"Clean-up-column-names-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Clean up column names</a></span></li><li><span><a href=\"#Cleaning-dates\" data-toc-modified-id=\"Cleaning-dates-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Cleaning dates</a></span></li><li><span><a href=\"#Adding-columns-with-year-and-month\" data-toc-modified-id=\"Adding-columns-with-year-and-month-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Adding columns with year and month</a></span></li></ul></li><li><span><a href=\"#Removing-columns\" data-toc-modified-id=\"Removing-columns-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Removing columns</a></span><ul class=\"toc-item\"><li><span><a href=\"#Save-your-data\" data-toc-modified-id=\"Save-your-data-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Save your data</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Clean data with Python Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this Jupyter Notebook! \n",
    "  \n",
    "Today you'll learn how to import a CSV file into a Jupyter Notebook, and how to clean up messy data. This notebook is part of the course Python for Journalists at [Learno.net](learno.net). The data used originally comes from [the Electoral Commission website](http://search.electoralcommission.org.uk/Search?currentPage=1&rows=10&sort=AcceptedDate&order=desc&tab=1&open=filter&et=pp&isIrishSourceYes=false&isIrishSourceNo=false&date=Reported&from=&to=&quarters=2018Q12&rptPd=3617&prePoll=false&postPoll=false&donorStatus=individual&donorStatus=tradeunion&donorStatus=company&donorStatus=unincorporatedassociation&donorStatus=publicfund&donorStatus=other&donorStatus=registeredpoliticalparty&donorStatus=friendlysociety&donorStatus=trust&donorStatus=limitedliabilitypartnership&donorStatus=impermissibledonor&donorStatus=na&donorStatus=unidentifiabledonor&donorStatus=buildingsociety&register=ni&register=gb&optCols=Register&optCols=IsIrishSource&optCols=ReportingPeriodName), but is edited for training purposes. The edited dataset is available on the Learno website. \n",
    "\n",
    "Remember: before you start working with data, make sure to create a copy of the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Jupyter Notebooks and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now you're looking at a Jupyter Notebook: an interactive, browser based programming environment. You can use these notebooks to program in R, Julia or Python - as you'll be doing later on. Read more about Jupyter Notebook in the [Jupyter Notebook Quick Start Guide](https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html). \n",
    "  \n",
    "To clean up our data, we'll be using Python and Pandas. Pandas is an open-source Python library - basically an extra toolkit to go with Python - that is designed for data analysis. Pandas is flexible, easy to use and has lots of useful functions built right in. Read more about Pandas and its features in [the Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/).\n",
    "\n",
    "**Notebook shortcuts**  \n",
    "\n",
    "Within Jupyter Notebooks, there are some shortcuts you can use. If you'll be using more notebooks for your data analysis in the future, you'll remember these shortcuts soon enough. :) \n",
    "\n",
    "* `esc` will take you into command mode\n",
    "* `a` will insert cell above\n",
    "* `b` will insert cell below\n",
    "* `shift then tab` will show you the documentation for your code\n",
    "* `shift and enter` will run your cell\n",
    "* ` d d` will delete a cell\n",
    "\n",
    "**Pandas dictionary**\n",
    "\n",
    "* **dataframe**: dataframe is Pandas speak for a table with a labeled y-axis, also known as an index. (The index usually starts at 0.)\n",
    "* **series**: a series is a list, a series can be made of a single column within a dataframe.\n",
    "\n",
    "Before we dive in, a little more about Jupyter Notebooks. Every notebooks is made out of cells. A cell can either contain Markdown text - like this one - or code. In the latter you can execute your code. To see what that means, type the following command in the next cell `print(\"hello world\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, if you're good to go, let dive in..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "Before we can work on our data, we need to import all libraries we'll need. In this case, we need to import the Pandas library. You can do that by typing in `import pandas as pd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`as pd` means that when you'll be writing code you can refer to the library by writing `pd` instead of `pandas`. It's just a little bit shorter and therefore more efficient - something programmers like a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this course, we'll be using data on donations done to British political parties. The data was originally downloaded from [the Electoral Commission website](http://search.electoralcommission.org.uk/Search?currentPage=1&rows=10&sort=AcceptedDate&order=desc&tab=1&open=filter&et=pp&isIrishSourceYes=false&isIrishSourceNo=false&date=Reported&from=&to=&quarters=2018Q12&rptPd=3617&prePoll=false&postPoll=false&donorStatus=individual&donorStatus=tradeunion&donorStatus=company&donorStatus=unincorporatedassociation&donorStatus=publicfund&donorStatus=other&donorStatus=registeredpoliticalparty&donorStatus=friendlysociety&donorStatus=trust&donorStatus=limitedliabilitypartnership&donorStatus=impermissibledonor&donorStatus=na&donorStatus=unidentifiabledonor&donorStatus=buildingsociety&register=ni&register=gb&optCols=Register&optCols=IsIrishSource&optCols=ReportingPeriodName).\n",
    "\n",
    "We are going to create a dataframe by importing a CSV with our data. Data can be imported using the following code: `df = pd.read_csv('filename.csv')`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/winnydejong/Desktop/results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you'll have a CSV file that doesn't use comma's to seperate values, but uses semi-colons or something else entirely. To import such a dataset change the code into:  \n",
    "`df = pd.read_csv('filename.csv', delimiter=\";\")`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When importing our data, we save the CSV file inside a dataframe that is called `df`. We can now explore the data by refering to the dataframe as `df`. It's important to 'get to know' your data, so you know what you're working with.\n",
    "\n",
    "Use `df.head(10)` to look at the first ten rows of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `df.tail(10)` to look at the last ten rows of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at a random sample of the data set, typ `df.sample(5)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to know what data types we're dealing with for each column in our dataframe\n",
    "\n",
    "Within Python different types of information, have different names. Using `df.dtypes` you can see what data type is in each column of the dataframe. \n",
    "\n",
    "**Most common data types**\n",
    "* **int**: short for integer, a number with no decimal\n",
    "* **float**: short for floating point, a number with at least one decimal. \n",
    "* **string**: usually a bit of text, if there are numbers in a string they are not recognized as such. Python will see a string as text.\n",
    "* **object**: usually a bit of text, if there are numbers in a string they are not recognized as such. Python will see a string as text.\n",
    "* **bool**: short for boolean, a binary data type, (true/false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the shape of the dataframe - the number of rows and columns - type `df.shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see a descriptive statistics summary of our data, including the median, average value for every column, type `df.describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, using `df.describe()`, the Value column is missing. What happened? The most interesting data is in there... Well, remember when we asked Python to give us the data type for each column using `df.dtypes`? \n",
    "\n",
    "Turns out, Python doesn't recognize the values in the Value column as numbers. Spoiler alert: that might have something to do with the comma's and pound-sign in that column. Guess what? It's time to do some data cleaning. \n",
    "\n",
    "# Clean data\n",
    "\n",
    "**Clean data to do list**\n",
    "- make sure that numerical values are recognized as such\n",
    "- dates are just objects, lets make Python recognize dates as dates\n",
    "- create new columns based on the date (like a column for year and month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning strings\n",
    "\n",
    "If we're going to analyse the data, we need the Value column to be recognized as float-numbers. (Floats, not ints since the Value column has numbers with decimals in there.)\n",
    "\n",
    "First, let's remove all of the pound-signs £... Type `df['ValueClean'] = df['Value'].str.replace('£', '')`. Now, what does this doe? It adds the column ValueClean, which is exactly the same as the column Value, but with every '£' replaced by nothing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['ValueClean'] = df['Value'].str.replace('£', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's heave a look at the first rows to see how we've done. Remember `df.head()`? ValueClean will be added at the right end of the table..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not done yet with this ValueClean column. We need to remove all comma's - Python doesn't like comma's or points for thousands, only for decimals. How would you remove all comma's in the ValueClean column without creating a new column? \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    "  \n",
    "The answer looks a lot like `df['ValueClean'] = df['Value'].str.replace('£', '')` but isn't exactly the same...\n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    "  \n",
    "Type `df['ValueClean'] = df['ValueClean'].str.replace(',', '')`, which will replace the column ValueClean with the column ValueClean where all comma's are replaced by nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['ValueClean'] = df['ValueClean'].str.replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if this did the trick. Use `df.dtypes` to see if the ValueClean column is now a float datatype..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Didn't work, huh? That's because we need to explicitly tell Python that the ValueClean column contains float numbers. We can use a Pandas function to do this - like all Pandas functions this one too starts with `pd.`: \n",
    "\n",
    "`df['ValueClean'] = pd.to_numeric(df['ValueClean'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['ValueClean'] = pd.to_numeric(df['ValueClean'])\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete columns\n",
    "\n",
    "Ok, now we got our Value column cleaned up in ValueClean; we actually no longer need to keep the original Value column. In Pandas removing or deleting a column is called 'dropping a column'. \n",
    "\n",
    "Also good to know: in Pandas, rows (horizontal) in a dataframe have axis=0, columns (vertical) have the first axis (axis=1). \n",
    "\n",
    "Knowing this, typing `df = df.drop('Value', 1)` should make sense. It means: the dataframe is the dataframe with the column Value dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Value', 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Value column gone, we can rename ValueClean to Value. Use the following command to do this: `df = df.rename(columns={'old_name': 'new_name'})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'ValueClean': 'Value'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up column names\n",
    "\n",
    "Since leading and trail spaces will always come back to haunt you in your data analysis nightmares, you want to make sure you get them out of your way before analysing your data. \n",
    "\n",
    "Let's see if there are any of these spaces in our column names, by typing `df.columns`, which will give us a list of all column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good to me... Let's check all donor names. Get a list of all donor names by using the following command: `df['columnname'].unique()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['DonorName'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, well, well, Mr Alun Ffred Jones  !! It going to take multiple steps to fix that: \n",
    "1. Let's make sure the DonorName column is a string.   \n",
    "Use `df['columnname'] = df['columnname'].astype(str)`\n",
    "2. Strip all strings in the column of leading and trail spaces.  \n",
    "Use `df['columnname'] = df['columnname'].map(str.strip)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['DonorName'] = df['DonorName'].astype(str)\n",
    "df['DonorName'] = df['DonorName'].map(str.strip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did that work? Let's see. `df['DonorName'].unique()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DonorName'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning dates\n",
    "\n",
    "Ok, so now we've only got to clean up our dates. We're going to use another python library: the datetime library contains some neat and handy datetime tools. Just what we need, type: `import datetime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have another look at some of our data before we start working on the date column. Use `df.head()`, `df.tail()` or my personal favorite `df.sample()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dates are in the AcceptedDate column. Let's make sure these dates are recognized as such. Use `df['AcceptedDate'] = pd.to_datetime(df['AcceptedDate'], format=\"%d/%m/%Y\")` to change the data type from object to date. Use `df.head()`, `df.tail()` or `df.sample()` to see if it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AcceptedDate'] = pd.to_datetime(df['AcceptedDate'], format=\"%d/%m/%Y\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, did the data type of the AcceptedDate column change? Use `df.dtypes` to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worked perfectly! :)   \n",
    "\n",
    "## Adding columns with year and month\n",
    "\n",
    "Now, let's create two new columns. One with the month and one with the years... Since Python now knows that the AcceptedDate column contains dates, we can use out-of-the-box functions from pandas and the datetime libraries. \n",
    "\n",
    "Creating a column with the years based on the AcceptedDate column, becomes as easy as `df['Year'] = pd.DatetimeIndex(df['AcceptedDate']).year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Year'] = pd.DatetimeIndex(df['AcceptedDate']).year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `df.head()`, `df.tail()` or `df.sample()` to see if it worked. Our new column will be added on the right side of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a column with all months is just as easy. `df['Month'] = pd.DatetimeIndex(df['AcceptedDate']).month` will do the trick, it means: in the dataframe called 'df', create a new column called 'Month' and fill it with months, which you should base on the date inside the column AcceptedDate of the dataframe df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Month'] = pd.DatetimeIndex(df['AcceptedDate']).month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `df.head()`, `df.tail()` or `df.sample()` to see if it worked. For something like this, i like to use the `df.sample()` function; it allows you to see if it worked with different values. \n",
    "\n",
    "Off course this new column too is  added on the right side of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing columns\n",
    "\n",
    "Our dataframe is quite big. Maybe we can remove some columns? Let's see how many columns we got... Use `df.columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of the columns 'ECRef', 'AccountingUnitName', 'AccountingUnitsAsCentralParty', 'IsSponsorship', 'RegulatedDoneeType', 'CompanyRegistrationNumber', 'Postcode', 'DonationType','NatureOfDonation', 'PurposeOfVisit', 'DonationAction', 'ReceivedDate', 'ReportedDate', 'IsReportedPrePoll', 'ReportingPeriodName', 'IsBequest', 'IsAggregation', 'RegulatedEntityId', 'AccountingUnitId', 'RegisterName', 'IsIrishSource', 'AcceptedDateClean'. \n",
    "\n",
    "Let's drop some columns! `df = df.drop('Value', 1)`\n",
    "\n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    ".  \n",
    "\n",
    "Or, maybe we should just tell the computer what columns we like to keep. Might be shorter. :) \n",
    "Use `dfMini = df[['RegulatedEntityName', 'AcceptedDate', 'DonorName', 'DonorStatus', 'Year', 'Month','Value', 'RegulatedEntityType', 'DonorId', 'CampaigningName']]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfMini = df[['RegulatedEntityName', 'AcceptedDate', 'DonorName', 'DonorStatus', 'Year', 'Month','Value', 'RegulatedEntityType', 'DonorId', 'CampaigningName']]\n",
    "dfMini.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your data\n",
    "\n",
    "Now that we've put all this work into cleaning our dataset, let's save a copy. Off course Pandas has a nifty command for that too. Use `dfMini.to_csv('filename.csv', encoding='utf8')`. \n",
    "\n",
    "Be ware: use a different name than the filename of the original data file, or it will be overwritten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfMini.to_csv('results_clean.csv', encoding='utf8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
